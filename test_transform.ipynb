{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_transformer as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'cat', 'is', 'acrossing', 'the', 'road']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"a cat is acrossing the road\"\n",
    "b = \"but the cat is running into the forest\"\n",
    "tf.word_seq(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# janome transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['英語', '単語', 'スペース', '区切る', 'れる', '分割', '簡単', '日本語', '難しい']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '英語は単語ごとにスペースで区切られているので分割するのが簡単だが、日本語は難しい。'\n",
    "pos = ['名詞','動詞','形容詞']\n",
    "stop = ['の','いる','する','ごと','こと','[',']']\n",
    "\n",
    "janome = tf.create_parser(parts_of_speech=pos,stop_words=stop)\n",
    "tf.word_seq(text, parser = janome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mecab transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['英語', '単語', 'スペース', '区切る', 'れる', '分割', '簡単', '日本語', '難しい']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab = tf.create_parser(worker='mecab', parts_of_speech=pos,stop_words=stop)\n",
    "tf.word_seq(text, parser = mecab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute similarity scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36363636363636365"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import text_similarity as sim\n",
    "from score import score \n",
    "from operator import methodcaller\n",
    "\n",
    "\"\"\"\n",
    "map : apply `func` to all members in a list\n",
    "reduce: aggregate all members in a list to a single value by applying `func` \n",
    "filter: filter a list to a sub-list whose members evaluated to be True by the `func`\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def copy_degree(doc1, doc2, method='jaccard', sent_cutoff=0.10, para_cutoff=0.10):\n",
    "\n",
    "    def _similar_to(x,y,formula='jaccard'):\n",
    "        \"\"\" alias to different methods  \n",
    "        \"\"\"\n",
    "        func = methodcaller(f'{formula}_similarity',x,y)\n",
    "        return func(sim)\n",
    "\n",
    "    def _word_seq(text):\n",
    "        return  tf.word_seq(text, parser=mecab)\n",
    " \n",
    "    doc1 = [ list(map(_word_seq, par)) for par in map(tf.sent_seq, tf.para_seq(doc1))] \n",
    "    doc2 = [ list(map(_word_seq, par)) for par in map(tf.sent_seq, tf.para_seq(doc2))] \n",
    "\n",
    "    data_para = []\n",
    "    for i in range(len(doc1)):\n",
    "        for j in range(len(doc2)):\n",
    "            p1, p2 = doc1[i], doc2[j]\n",
    "            data_sent = []\n",
    "            for ii in range (len(p1)):\n",
    "                for jj in range(len(p2)):\n",
    "                    data_sent += [(ii,jj, _similar_to(p1[ii], p2[jj], formula=method))]\n",
    "\n",
    "            data_para += [(i,j, score(data_sent,cutoff=sent_cutoff))]    \n",
    "            \n",
    "    return score(data_para, cutoff=para_cutoff )       \n",
    "\n",
    "\n",
    "file1 = 'datasets/ishida/d20.txt'\n",
    "file2 = 'datasets/ishida/d40.txt'\n",
    "\n",
    "with open(file1, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "     text1 = f.read()\n",
    "with open(file2, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    text2 = f.read()\n",
    "    \n",
    "copy_degree(text1,text2, sent_cutoff=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JACCARD\n",
      "0.029 (d1.txt & d20.txt)\n",
      "0.091 (d1.txt & d40.txt)\n",
      "0.151 (d1.txt & d60.txt)\n",
      "0.227 (d1.txt & d80.txt)\n",
      "0.413 (d20.txt & d40.txt)\n",
      "0.193 (d20.txt & d60.txt)\n",
      "0.096 (d20.txt & d80.txt)\n",
      "0.212 (d40.txt & d60.txt)\n",
      "0.154 (d40.txt & d80.txt)\n",
      "0.194 (d60.txt & d80.txt)\n",
      "SIMPSON\n",
      "0.512 (d1.txt & d20.txt)\n",
      "0.490 (d1.txt & d40.txt)\n",
      "0.511 (d1.txt & d60.txt)\n",
      "0.556 (d1.txt & d80.txt)\n",
      "0.706 (d20.txt & d40.txt)\n",
      "0.619 (d20.txt & d60.txt)\n",
      "0.576 (d20.txt & d80.txt)\n",
      "0.587 (d40.txt & d60.txt)\n",
      "0.556 (d40.txt & d80.txt)\n",
      "0.569 (d60.txt & d80.txt)\n",
      "LEVENSHTEIN\n",
      "0.037 (d1.txt & d20.txt)\n",
      "0.080 (d1.txt & d40.txt)\n",
      "0.134 (d1.txt & d60.txt)\n",
      "0.210 (d1.txt & d80.txt)\n",
      "0.238 (d20.txt & d40.txt)\n",
      "0.136 (d20.txt & d60.txt)\n",
      "0.091 (d20.txt & d80.txt)\n",
      "0.168 (d40.txt & d60.txt)\n",
      "0.137 (d40.txt & d80.txt)\n",
      "0.174 (d60.txt & d80.txt)\n"
     ]
    }
   ],
   "source": [
    "dataset = 'datasets/ishida'\n",
    "files=['d1.txt', 'd20.txt', 'd40.txt', 'd60.txt', 'd80.txt']\n",
    "methods = ['jaccard','simpson','levenshtein']\n",
    "n = len(files)\n",
    "for m in methods:\n",
    "    print(m.upper())\n",
    "    for i in range(n-1):\n",
    "        for j in range(i+1,n):\n",
    "            file1, file2=f'{dataset}/{files[i]}', f'{dataset}/{files[j]}'\n",
    "            with open(file1, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "                text1 = f.read()\n",
    "            with open(file2, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "                text2 = f.read()\n",
    "            degree = copy_degree(text1,text2, method=m, sent_cutoff=0.1,para_cutoff=0.08)\n",
    "            print(f'{degree:.3f} ({files[i]} & {files[j]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
